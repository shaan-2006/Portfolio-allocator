# -*- coding: utf-8 -*-
"""portfolio1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bo9kGG_SnJaYa4g6eQ23EFmVEAjYVky2
"""

# Cell 1: Imports and Data Loading
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.model_selection import train_test_split

# Configuration
TRAIN_START, TRAIN_END = "2015-01-01", "2025-08-10"
BACKTEST_START, BACKTEST_END = "2025-08-11", "2025-08-18"
INITIAL_CAPITAL = 167_000
RF = 0.06/52  # Risk-free weekly return

# Define ticker universe (example list, add your tickers as needed)
tickers = [
    "ADANIENT.NS", "ADANIPORTS.NS", "APOLLOHOSP.NS", "ASIANPAINT.NS", "AXISBANK.NS","BAJAJ-AUTO.NS", "BAJFINANCE.NS", "BAJAJFINSV.NS", "BEL.NS", "BHARTIARTL.NS",
    "BPCL.NS", "BRITANNIA.NS", "CIPLA.NS", "COALINDIA.NS", "DIVISLAB.NS","DRREDDY.NS", "EICHERMOT.NS", "GRASIM.NS", "HCLTECH.NS", "HDFCBANK.NS",
    "HDFCLIFE.NS", "HEROMOTOCO.NS", "HINDALCO.NS", "HINDUNILVR.NS", "ICICIBANK.NS","INDUSINDBK.NS", "INFY.NS", "ITC.NS", "JSWSTEEL.NS", "KOTAKBANK.NS",
    "LT.NS", "LTIM.NS", "M&M.NS", "MARUTI.NS", "NESTLEIND.NS", "NTPC.NS","ONGC.NS", "POWERGRID.NS", "RELIANCE.NS", "SBILIFE.NS", "SBIN.NS",
    "SUNPHARMA.NS", "TATAMOTORS.NS", "TATASTEEL.NS", "TCS.NS","TECHM.NS", "TITAN.NS", "ULTRACEMCO.NS", "WIPRO.NS","ABB.NS", "ADANIENSOL.NS", "ADANIGREEN.NS", "ADANIPOWER.NS",
    "AMBUJACEM.NS", "BAJAJHLDNG.NS", "BAJAJHFL.NS", "BANKBARODA.NS","BOSCHLTD.NS", "CANBK.NS", "CGPOWER.NS", "CHOLAFIN.NS", "DABUR.NS",
    "DLF.NS", "DMART.NS", "GAIL.NS", "GODREJCP.NS", "HAVELLS.NS", "HAL.NS","HYUNDAI.NS", "ICICIGI.NS", "ICICIPRULI.NS", "INDHOTEL.NS", "IOC.NS",
    "INDIGO.NS", "NAUKRI.NS", "IRFC.NS", "JINDALSTEL.NS", "JSWENERGY.NS", "LICI.NS", "LODHA.NS", "PIDILITIND.NS", "PFC.NS", "PNB.NS", "RECLTD.NS",
    "MOTHERSON.NS", "SHREECEM.NS", "SIEMENS.NS", "SWIGGY.NS", "TATAPOWER.NS","TORNTPHARM.NS", "TVSMOTOR.NS", "UNITDSPR.NS", "VBL.NS", "VEDL.NS",
    "ZYDUSLIFE.NS", "TATACOMM.NS", "APOLLOTYRE.NS", "ABFRL.NS", "MANKIND.NS",
    "SAIL.NS", "DIXON.NS", "GMRAIRPORT.NS", "KALYANKJIL.NS", "NYKAA.NS","AUROPHARMA.NS", "BHARATFORG.NS", "IGL.NS", "MRF.NS", "BIOCON.NS",
    "SRF.NS", "IDEA.NS", "NATIONALUM.NS", "POLYCAB.NS", "PETRONET.NS", "COLPAL.NS", "ACC.NS", "MAXHEALTH.NS", "TORNTPOWER.NS", "INDUSTOWER.NS",
    "ESCORTS.NS", "GLENMARK.NS", "ALKEM.NS", "VOLTAS.NS", "OIL.NS","SONACOMS.NS", "PIIND.NS", "LUPIN.NS", "UPL.NS", "COFORGE.NS",
    "IRB.NS", "BANDHANBNK.NS", "NMDC.NS", "MAZDOCK.NS", "VMM.NS",
    "PATANJALI.NS", "IRCTC.NS", "CONCOR.NS", "MPHASIS.NS", "INDIANB.NS",
    "ASTRAL.NS", "PREMIERENE.NS", "ASHOKLEY.NS", "KPITTECH.NS", "BDL.NS",
    "IDFCFIRSTB.NS", "CUMMINSIND.NS", "JUBLFOOD.NS", "OFSS.NS", "PERSISTENT.NS",
    "MFSL.NS", "LICHSGFIN.NS", "PAYTM.NS", "ATGL.NS", "TATAELXSI.NS",
    "HINDZINC.NS", "YESBANK.NS", "FEDERALBNK.NS", "APLAPOLLO.NS", "UNIONBANK.NS",
    "EXIDEIND.NS", "TATATECH.NS", "RVNL.NS", "HINDPETRO.NS", "OBEROIRLTY.NS",
    "COCHINSHIP.NS", "BSE.NS", "NHPC.NS", "LTF.NS", "MUTHOOTFIN.NS",
    "MARICO.NS", "POLICYBZR.NS", "IREDA.NS", "PAGEIND.NS", "SOLARINDS.NS",
    "BANKINDIA.NS", "HDFCAMC.NS", "GODREJPROP.NS", "BHEL.NS", "SJVN.NS",
    "MOTILALOFS.NS", "NTPCGREEN.NS", "SUPREMEIND.NS", "BHARTIHEXA.NS",
    "SBICARD.NS", "AUBANK.NS", "M&MFIN.NS", "PRESTIGE.NS", "MAHABANK.NS",
    "ABCAPITAL.NS", "OLAELEC.NS", "HUDCO.NS", "WAAREEENER.NS", "PHOENIXLTD.NS",
    "TIINDIA.NS", "SUZLON.NS", "SAGILITY.NS", "RITES.NS", "PGEL.NS",
    "WELCORP.NS", "KAYNES.NS", "INOXWIND.NS", "GESHIP.NS", "TRITURBINE.NS",
    "FSL.NS", "AMBER.NS", "HBLENGINE.NS", "NEULANDLAB.NS", "MGL.NS",
    "LAURUSLABS.NS", "DATAPATTNS.NS", "SHYAMMETL.NS", "IEX.NS", "ZENSARTECH.NS",
    "ATUL.NS", "RAMCOCEM.NS", "ASTERDM.NS", "BSOFT.NS", "AFFLE.NS",
    "DELHIVERY.NS", "TATACHEM.NS", "CASTROLIND.NS", "IKS.NS", "GSPL.NS",
    "CESC.NS", "AADHARHFC.NS", "TEJASNET.NS", "CYIENT.NS", "POONAWALLA.NS",
    "AEGISLOG.NS", "FIRSTCRY.NS", "HINDCOPPER.NS", "LALPATHLAB.NS", "ABREL.NS",
    "FIVESTAR.NS", "PCBL.NS", "CREDITACC.NS", "PPLPHARMA.NS", "SIGNATURE.NS",
    "AARTIIND.NS", "BLS.NS", "WELSPUNLIV.NS", "BATAINDIA.NS", "GODFRYPHLP.NS",
    "KARURVYSYA.NS", "GRSE.NS", "CAMS.NS", "INDIAMART.NS", "NAVINFLUOR.NS",
    "GODIGIT.NS", "NEWGEN.NS", "HFCL.NS", "SWANENERGY.NS", "TTML.NS",
    "JWL.NS", "PVRINOX.NS", "ARE&M.NS", "AFCONS.NS", "KEC.NS", "IGIL.NS",
    "ITI.NS", "PEL.NS", "ACE.NS", "NATCOPHARM.NS", "RAILTEL.NS", "KFINTECH.NS",
    "CDSL.NS", "SONATSOFTW.NS", "JBMA.NS", "KPIL.NS", "ZENTEC.NS",
    "CROMPTON.NS", "TRIDENT.NS", "DEVYANI.NS", "IIFL.NS", "ANGELONE.NS",
    "TITAGARH.NS", "MCX.NS", "IRCON.NS", "RKFORGE.NS", "BEML.NS",
    "CHAMBLFERT.NS", "IFCI.NS", "HSCL.NS", "PNBHOUSING.NS", "IDBI.NS",
    "ANANTRAJ.NS", "NUVAMA.NS", "RADICO.NS", "RPOWER.NS", "MANAPPURAM.NS",
    "NBCC.NS", "BRIGADE.NS", "NCC.NS", "REDINGTON.NS", "NH.NS"
]

# Download OHLCV data (auto-adjusted for splits/dividends)
data = yf.download(tickers, start=TRAIN_START, end=BACKTEST_END, auto_adjust=True, progress=False)
close = data['Close'].ffill()
high = data['High'].ffill()
low  = data['Low'].ffill()
volume = data['Volume'].fillna(0)

# Compute daily and weekly returns
rets_daily = close.pct_change().dropna()
prices_weekly = close.resample("W-MON").last().ffill()
rets_weekly = prices_weekly.pct_change().dropna()

# Benchmark index (Nifty 50) and forecast feature (ARIMA on weekly returns)
bench = yf.download("^NSEI", start=TRAIN_START, end=BACKTEST_END, auto_adjust=True, progress=False)['Close']
bench_weekly = bench.resample("W-MON").last().pct_change().dropna()
from statsmodels.tsa.arima.model import ARIMA
arima_model = ARIMA(bench_weekly.loc[TRAIN_START:TRAIN_END], order=(1,0,1)).fit()
arima_forecast = arima_model.forecast(steps=len(bench_weekly) - len(arima_model.fittedvalues))
feat_arima = pd.Series(arima_forecast.values, index=bench_weekly.index[-len(arima_forecast):], name="Bench_ARIMA")

# Cell 2: Feature Engineering
from sklearn.preprocessing import StandardScaler

# 1) Technical Indicators (momentum, RSI, volatility)
def add_technical_indicators(prices):
    mom = prices.pct_change(12).add_suffix("_mom")
    # Exponential moving average cross
    ema50, ema200 = prices.ewm(span=50).mean(), prices.ewm(span=200).mean()
    cross = (ema50 - ema200).add_suffix("_cross")
    # Bollinger Bands %B
    mid = prices.rolling(20).mean()
    std = prices.rolling(20).std()
    bbp = ((prices - mid) / (2 * std)).add_suffix("_bbp")
    # RSI
    delta = prices.diff()
    up = delta.clip(lower=0).rolling(14).mean()
    down = -delta.clip(upper=0).rolling(14).mean()
    rsi = (100 - 100/(1 + up/down)).add_suffix("_rsi")
    # MACD
    ema12, ema26 = prices.ewm(span=12).mean(), prices.ewm(span=26).mean()
    macd = (ema12 - ema26).add_suffix("_macd")
    sig = macd.ewm(span=9).mean().add_suffix("_macd_sig")
    return pd.concat([mom, cross, bbp, rsi, macd, sig], axis=1)

tech = add_technical_indicators(prices_weekly)

# 2) Volume Indicators
vol_rolling_mean = volume.resample("W-MON").mean().ffill()
volatility = rets_weekly.rolling(20).std().add_suffix("_volatility")  # 20-week volatility
volume_ratio = (vol_rolling_mean / vol_rolling_mean.rolling(20).mean()).add_suffix("_vol_ratio")

# 3) Macro and Breadth Features
# Sentiment (optional placeholder; requires API key and network access)
from textblob import TextBlob
def get_news_sentiment(date):
    # Placeholder function: implement using an API (e.g., NewsAPI) if available
    return 0.0  # e.g., average polarity of news headlines for given date

sentiment_daily = pd.Series({d: get_news_sentiment(d) for d in rets_daily.index})
sentiment_weekly = sentiment_daily.resample("W-MON").mean().reindex(rets_weekly.index).fillna(0)

# Market breadth (fraction of stocks above 50-week MA)
breadth = (prices_weekly > prices_weekly.rolling(50).mean()).sum(axis=1) / prices_weekly.shape[1]

# 4) Combine all features
features = pd.concat([
    tech,
    volatility,
    volume_ratio,
    sentiment_weekly.rename("sentiment"),
    feat_arima.reindex(prices_weekly.index).fillna(0),
    breadth.rename("breadth")
], axis=1).fillna(0)

# Ensure feature and return indices align
features = features.loc[rets_weekly.index]
X = features.values
Y = rets_weekly.values  # Y is (weeks x tickers) of weekly returns

# Cell 3: Model Training with Ensemble of RF, XGB, and a Neural Network
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.neural_network import MLPRegressor
import xgboost as xgb

# Split into training and backtest sets
mask_train = (rets_weekly.index >= TRAIN_START) & (rets_weekly.index <= TRAIN_END)
mask_test  = (rets_weekly.index >= BACKTEST_START) & (rets_weekly.index <= BACKTEST_END)
X_train, X_test = features.loc[mask_train].values, features.loc[mask_test].values
Y_train, Y_test = rets_weekly.loc[mask_train].values, rets_weekly.loc[mask_test].values

# Standardize features for neural network
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# 1) Random Forest (multi-output)
rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
rf_multi = MultiOutputRegressor(rf).fit(X_train_scaled, Y_train)

# 2) XGBoost (multi-output via wrapper)
xgb_reg = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, subsample=0.8, random_state=42, n_jobs=-1)
xgb_multi = MultiOutputRegressor(xgb_reg).fit(X_train_scaled, Y_train)

# 3) Multi-Layer Perceptron (neural network)
mlp = MLPRegressor(hidden_layer_sizes=(128,64), activation='relu', solver='adam',
                   max_iter=200, random_state=42)
mlp_multi = MultiOutputRegressor(mlp).fit(X_train_scaled, Y_train)

# 4) Ensemble Predictions (average of models)
pred_rf  = rf_multi.predict(X_test_scaled)
pred_xgb = xgb_multi.predict(X_test_scaled)
pred_mlp = mlp_multi.predict(X_test_scaled)
predictions = (pred_rf + pred_xgb + pred_mlp) / 3  # (weeks x tickers)

# Cell 4: Portfolio Allocation Based on Predictions
pred_df = pd.DataFrame(predictions, index=rets_weekly.loc[mask_test].index, columns=rets_weekly.columns)
rets_df = pd.DataFrame(Y_test, index=pred_df.index, columns=rets_weekly.columns)

# Compute portfolio weights: allow long and short (normalize by sum of absolute preds)
weights = pred_df.div(pred_df.abs().sum(axis=1), axis=0).fillna(0)

# Calculate weekly portfolio return
weekly_ret = (weights * rets_df).sum(axis=1)
port_vals = (1 + weekly_ret).cumprod() * INITIAL_CAPITAL

print("Backtest Portfolio Final Value: â‚¹{:.2f}".format(port_vals.iloc[-1]))

# Cell 5: Generate Trade Tickets with Slippage and Stop-Loss
prices_daily = close.ffill()
slip = 0.0015
fee_buy = 0.001
fee_sell = 0.001

trade_rows = []
for date in weights.index:
    w = weights.loc[date]
    nav_start = port_vals.loc[date - pd.offsets.Week()] if date != weights.index[0] else INITIAL_CAPITAL
    # Get last close price before this date
    prev_idx = prices_daily.index.get_indexer([date], method="pad")[0]
    entry_price = prices_daily.iloc[prev_idx]
    for tkr in weights.columns:
        wt = w[tkr]
        if wt == 0 or np.isnan(wt):
            continue
        price = entry_price[tkr]
        if price <= 0 or not np.isfinite(price):
            continue
        alloc = abs(wt) * nav_start
        if wt > 0:
            # Long position
            buy_price = price * (1 + slip + fee_buy)
            shares = int(np.floor(alloc / buy_price))
            if shares <= 0:
                continue
            rupee_val = shares * buy_price
            stop_loss = round(buy_price * 0.95, 2)
        else:
            # Short position
            sell_price = price * (1 - slip - fee_sell)
            shares = int(np.floor(alloc / price))
            if shares <= 0:
                continue
            rupee_val = shares * price
            stop_loss = round(price * 1.05, 2)
        trade_rows.append({
            "Date": date.date(),
            "Ticker": tkr,
            "Position": "Long" if wt>0 else "Short",
            "Entry_Price": round(buy_price if wt>0 else sell_price, 2),
            "Shares": shares,
            "Rupee_Value": round(rupee_val, 2),
            "Weight(%)": round(wt*100, 2),
            "Stop_Loss": stop_loss
        })

df_trades = pd.DataFrame(trade_rows)
df_trades.sort_values(["Date","Weight(%)"], ascending=[True,False], inplace=True)
print("Trade Ticket (first rows):")
print(df_trades.head(10))

